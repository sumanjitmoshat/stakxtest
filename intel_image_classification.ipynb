{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "intel_image_classification.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMWbHai0oN95pINoV9QEMLV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumanjitmoshat/stakxtest/blob/master/intel_image_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhtdDO8zXQmA"
      },
      "source": [
        "### importing required packages\n",
        "import pathlib\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import regularizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6ogixTYXSC2"
      },
      "source": [
        "# connecting with kaggle to download the dataset \n",
        "! pip install -q kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets list\n",
        "!kaggle datasets download -d puneet6060/intel-image-classification\n",
        "! mkdir train\n",
        "! unzip intel-image-classification.zip -d train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3miOqUO-TK1b"
      },
      "source": [
        "### setting path for training and test dataset\n",
        "data_dir_train= pathlib.Path(\"train/seg_train/seg_train\")\n",
        "data_dir_test= pathlib.Path(\"train/seg_test/seg_test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C37EUAevXNP1"
      },
      "source": [
        "### checking the count in train and test\n",
        "image_count_train = len(list(data_dir_train.glob('*/*.jpg')))\n",
        "image_count_test = len(list(data_dir_test.glob('*/*.jpg')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyixMQv1YCdA"
      },
      "source": [
        "### printing image count\n",
        "print(image_count_train)\n",
        "print(image_count_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b4ESWllcJHG"
      },
      "source": [
        "### setting the batch size to 32 and image height and \n",
        "batch_size = 32\n",
        "img_height = 150\n",
        "img_width = 150"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvI0OIqpg0h-"
      },
      "source": [
        "### intializing the training dataset , keeping 20 % of the data as validaiton set\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=data_dir_train,\n",
        "    seed=123,\n",
        "    validation_split=0.2,\n",
        "    subset= 'training',\n",
        "    image_size=(img_height,img_width),\n",
        "    batch_size= batch_size\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Mx_ucXKi0Fp"
      },
      "source": [
        "### initializing the validation dataset \n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=data_dir_test,\n",
        "    seed=123,\n",
        "    validation_split=0.2,\n",
        "    subset= 'validation',\n",
        "    image_size=(img_height,img_width),\n",
        "    batch_size= batch_size\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcfRLo8njYLv"
      },
      "source": [
        "### checking the class names\n",
        "class_names = train_ds.class_names\n",
        "print(class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdbHrDgZjqAp"
      },
      "source": [
        "### printing first 9 images from the dataset\n",
        "plt.figure(figsize=(10,10))\n",
        "for images,labels in train_ds.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3,3,1+i)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01hCxS-0opLB"
      },
      "source": [
        "AUTOTUNE finds the optimal CPU allocation across all parameters . train_ds.cache() keeps the images in memory after they've been loaded off disk during the first epoch . train_ds.prefetch overlaps data pre processing and model execution while training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhAVL_9cvXbN"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdK4Csoop-1G"
      },
      "source": [
        "Creating the model with 3 convolution layer and 2 dense layer after flattening . The inputs are tensor of 150 * 150 * 3 . They are being normalized in the input as pixel value can take a value in range of 0 to 255"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aakPfsZ_cgK5"
      },
      "source": [
        "num_classes = 6\n",
        "epochs=20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BKtijSqw0U5"
      },
      "source": [
        "model = Sequential([\n",
        "            layers.experimental.preprocessing.Rescaling(1./255,input_shape=(img_height,img_width,3)),\n",
        "            layers.Conv2D(16,3,padding='same',activation='relu'),\n",
        "            layers.MaxPooling2D(),\n",
        "            layers.Conv2D(32,3,padding='same',activation='relu'),\n",
        "            layers.MaxPooling2D(),\n",
        "            layers.Conv2D(64,3,padding='same',activation='relu'),\n",
        "            layers.MaxPooling2D(),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(128,activation='relu'),\n",
        "            layers.Dense(num_classes,activation='softmax')\n",
        "                    \n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gybUPQmtrCec"
      },
      "source": [
        "Using adam(adagrad+rmsprop) as optimizer and SparseCategoricalCrossEntropy as the loss function. And using accuracy as the metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOnKUJxG2cvq"
      },
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy']\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju4er4L54hNc"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM6rmS2w4uY5"
      },
      "source": [
        "### training the model for 20 epochs\n",
        "\n",
        "history = model.fit(train_ds,validation_data=val_ds,epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvPV1Mzt5BLg"
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss= history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(epochs_range,acc,label='Training Accuracy')\n",
        "plt.plot(epochs_range,val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(epochs_range,loss,label='Training Loss')\n",
        "plt.plot(epochs_range,val_loss, label='Validation Loss')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Loss')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QGvAzVfyPUQ"
      },
      "source": [
        "As we can see the model is clearly overfitting so introducing dropout to reduce overfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lze8r1gh3YEz"
      },
      "source": [
        "earlystop= EarlyStopping(monitor='val_accuracy', patience=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHl3d0fCoOnd"
      },
      "source": [
        "model_dropout = Sequential([\n",
        "            data_augmentation,\n",
        "            layers.experimental.preprocessing.Rescaling(1./255,input_shape=(img_height,img_width,3)),\n",
        "            layers.Conv2D(16,3,padding='same',activation='relu'),\n",
        "            layers.MaxPooling2D(),\n",
        "            layers.Conv2D(32,3,padding='same',activation='relu'),\n",
        "            layers.MaxPooling2D(),\n",
        "            layers.Conv2D(64,3,padding='same',activation='relu'),\n",
        "            layers.MaxPooling2D(),\n",
        "            layers.Dropout(0.2),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(128,activation='relu'),\n",
        "            layers.Dropout(0.2),\n",
        "            layers.Dense(num_classes,activation='softmax')\n",
        "                    \n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yKPAREqAk5b"
      },
      "source": [
        "model_dropout.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy']\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HevubQCkApGh"
      },
      "source": [
        "model_dropout.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPS71DZhAq0c"
      },
      "source": [
        "history_dropout = model_dropout.fit(train_ds,validation_data=val_ds,epochs=epochs,callbacks=[earlystop])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXQsYeAwAypV"
      },
      "source": [
        "acc = history_dropout.history['accuracy']\n",
        "val_acc = history_dropout.history['val_accuracy']\n",
        "\n",
        "loss= history_dropout.history['loss']\n",
        "val_loss = history_dropout.history['val_loss']\n",
        "\n",
        "epochs_range = range(15)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(epochs_range,acc,label='Training Accuracy')\n",
        "plt.plot(epochs_range,val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(epochs_range,loss,label='Training Loss')\n",
        "plt.plot(epochs_range,val_loss, label='Validation Loss')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Loss')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Hu7Gip5LsU8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}